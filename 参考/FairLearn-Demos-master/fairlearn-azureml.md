## 用Fairlearn和Azure机器学习缓解不公平现象。

### 1.1 目录

简介
加载数据
训练一个未解决的模型
使用GridSearch进行缓解
将公平仪表板上传到Azure
	注册模型
	计算公平性指标
	上传到Azure
结语

### 1.2介绍

​		本笔记本展示了如何使用Fairlearn（一个开源的公平性评估和不公平性缓解包）和Azure机器学习工作室来处理一个二进制分类问题。这个例子使用了著名的成人人口普查数据集。在本笔记本中，我们将把它作为一个贷款决策问题。我们将假装标签表示每个人在过去是否偿还了贷款。我们将使用这些数据来训练一个预测器，以预测以前未见过的个人是否会偿还贷款。我们的假设是，模型的预测被用来决定是否应该向个人提供贷款。其目的纯粹是为了说明包括公平性仪表盘在内的工作流程--特别是，我们不包括对考虑机器学习中的公平性时出现的详细问题的全面讨论。关于此类讨论，请参考Fairlearn网站。
​		我们将应用Fairlearn软件包中的网格搜索算法，使用一个特定的公平概念，称为人口均等性。这将产生一组模型，我们将在本地和Azure机器学习工作室的仪表板上查看这些模型。

### 1.3 加载数据

​		我们将把每个人的性别作为一个受保护的属性（0表示女性，1表示男性），在这种特殊情况下，我们将把这个属性分离出来，从主要数据中删除（这并不总是最好的选择--见Fairlearn网站的进一步讨论）。我们也将种族列分离出来，但我们不会在此基础上进行任何缓解。最后，我们执行一些标准的数据预处理步骤，将数据转换成适合ML算法的格式。	

### 1.4 训练未受公平影响的模型

​		所以我们有一个比较点，我们首先在原始数据上训练一个模型（特别是scikit-learn的逻辑回归），不应用任何缓解算法。
​		当我们选择 "性别 "作为敏感特征时，观察准确率的差异，我们看到男性的错误率是女性的三倍左右。更有趣的是机会方面的差异--男性获得贷款的比率是女性的三倍。
​		尽管我们从训练数据中删除了这个特征，但我们的预测器仍然有基于性别的歧视。这表明，在拟合预测器时简单地忽略一个受保护的属性，很少能消除不公平现象。一般来说，会有足够的其他特征与被移除的属性相关，从而导致不同的影响。

### 1.5 使用GridSearch的缓解措施

​		Fairlearn中的GridSearch类实现了Agarwal等人2018年的指数化梯度还原的简化版本。用户提供了一个标准的ML估计器，它被视为一个黑盒子--对于这个简单的例子，我们将使用scikit-learn的逻辑回归估计器。GridSearch的工作方式是生成一连串的重标和重权，并为每个人训练一个预测器。
​		在这个例子中，我们指定人口奇偶性（在性别这一受保护的属性上）作为公平性的指标。人口均等要求个人获得的机会（本例中的贷款）与受保护阶层的成员身份无关（即，女性和男性应该以相同的比率获得贷款）。在这个例子中，我们使用这个指标是为了简单起见；只有在仔细检查了模型使用的更广泛的背景之后，才能选择适当的公平指标。

1.6

### 1.7 总结

​		在这个笔记本中，我们演示了如何使用Fairlearn的GridSearch算法来生成模型集合，然后在Azure机器学习工作室的公平性仪表盘中展示它们。请记住，这本笔记本并没有试图讨论许多考虑因素，这些因素应该是任何缓解不公平的方法的一部分。Fairlearn网站提供了这方面的讨论