# <cneter>可信机器学习期末实验报告

| 实验课程：可信机器学习                   | 年级：2018                   |
| ---------------------------------------- | ---------------------------- |
| 实验名称：UCI 信用卡数据集的二元分类分析 | 姓名：龙旷飞、李泽浩、杨添程 |
| 实验编号：期末Project                    | 学号：                       |
| 指导教师：赵静                           | 组号：                       |



## 一、实验背景

​		本次项目中我们模拟了贷款决策中出现的准确性差异问题。具体来说，我们考虑的情况是，算法工具根据历史数据进行训练，其对贷款申请人的预测被用于对申请人做出决定。关于涉及信用额度决策中基于性别的歧视的例子，请看[这里](https://www.nytimes.com/2019/11/10/business/Apple-credit-card-investigation.html)。
​		我们使用[UCI信用卡数据集](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)。为了这个练习，我们修改了原始的UCI数据集：我们引入了一个合成特征，该特征对女性客户有很强的预测能力，但对男性申请人没有信息。我们拟合了各种预测客户违约的模型。我们表明，一个没有公平意识的训练算法可以导致一个预测器对女性的准确率远远高于男性，而且简单地从训练中删除敏感特征（在这种情况下为性别）是不够的。然后我们使用`ThresholdOptimizer`和`GridSearch`来改进模型。		



## 二、理论知识

##### 1.机器学习的公平性

​		人工智能和机器学习系统可能会表现出不公平的行为。 定义不公平行为的一种方法是按其损害或对人的影响来定义。 人工智能系统可以造成许多类型的损害。人工智能造成的两种常见损害是：

- 对分配的损害：AI 系统会对特定群体提供或拒绝提供机会、资源或信息。 示例包括招聘、学校招生和贷款。在这些场景中，某个模型在特定人群中挑选优秀候选人的能力可能要强于在其他人群中进行挑选的能力。

- 对服务质量的损害：人工智能系统针对一个人群的工作质量没有针对另一个人群的工作质量好。 例如，语音识别系统针对女性的工作质量可能没有针对男性的工作质量好。

  为了减少人工智能系统中的不公平行为，你必须评估并缓解这些损害。



##### 2.通过Fairlearn进行公平性缓解和评估

Fairlearn 是一个开源 Python 包，它使机器学习系统开发者能够评估其系统的公平性并缓解不公平性。

（⚠️注意：公平性是一个社会性的技术难题。 公平性的许多方面（例如公正和正当程序）没有通过量化的公平性指标进行捕获。 另外，许多量化的公平性指标无法同时得到满足。 Fairlearn 开源包的目标是使人类能够评估不同的影响和缓解策略。 最终，由构建人工智能和机器学习模型的人类用户负责根据其应用场景进行权衡。）

Fairlearn 开源包有两个组件：

- 评估仪表板：这是一个 Jupyter 笔记本小组件，用于评估模型的预测如何影响不同的群体。 它还支持使用公平性和性能指标对多个模型进行比较。
- 缓解算法：一组用于在二元分类和回归中缓解不公平性的算法。

借助这些组件，数据科学家和业务主管能够在公平性和性能之间进行权衡，并选择最适合其需求的缓解策略。



##### 3.评估机器学习模型中的公平性

​		在 Fairlearn 开源包中，公平性通过一种称为“群体公平性”的方法进行概念化，该方法会询问以下问题：哪些群体有遭受损害的风险？ 相关群体（也称为亚群体）是通过 **敏感特征** 或敏感特性定义的。 敏感特征作为名为 `sensitive_features` 的矢量或矩阵传递给 Fairlearn 开源包中的估算器。 此术语的意思是，系统设计者在评估群体公平性时应该对这些特征敏感。
​		需要注意的是，这些特征是否包含由于私有数据而产生的隐私影响。 但是“敏感”并不意味着这些特征不应被用来进行预测。
（⚠️注意：公平性评估并非纯粹的技术练习。 Fairlearn 开源包可帮助你评估模型的公平性，但不会为你执行评估。 Fairlearn 开源包会帮助识别量化指标以评估公平性，但开发人员还必须执行定性分析来评估其自己的模型的公平性。 上面所述的敏感特征是此类定性分析的一个示例。）
​		在评估阶段，将通过差异指标对公平性进行量化。 **差异指标** 能够以比率或差值的形式评估并比较模型在不同群体中的行为。 Fairlearn 开源包支持两类差异指标：

- 模型性能差异：这些指标集计算所选性能指标的值在不同子群体之间的差异。 示例包括：
  - 准确率差异
  - 错误率差异
  - 精度差异
  - 召回率差异
  - MAE 差异
  - 许多其他差异
- 选择率差异：此指标包含不同子群体之间的选择率差异。 此差异的一个示例是贷款批准率差异。 选择率是指每个分类中归类为 1 的数据点所占的比例（在二元分类中）或者指预测值的分布（在回归中）。



##### 4.减少机器学习模型中的不公平性

​		Fairlearn 开源包包括了各种不公平性缓解算法。 这些算法支持对预测器行为的一组约束（称为 **奇偶校验约束** 或条件）。 奇偶校验约束要求预测器行为的某些方面在敏感特征所定义的群体（例如不同的种族）之间具有可比性。 Fairlearn 开源包中的缓解算法使用此类奇偶校验约束来缓解所观察到的公平性问题。
（⚠️注意：缓解模型中的不公平性意味着降低不公平性，但这种技术上的缓解无法完全消除此不公平性。 Fairlearn 开源包中的不公平性缓解算法可提供建议的缓解策略，以帮助减少机器学习模型中的不公平性，但它们并不是用来完全消除不公平性的解决方案。 每个特定开发人员的机器学习模型可能还有其他应考虑的奇偶校验约束或条件。 使用 Azure 机器学习的开发人员必须自行确定，缓解措施是否充分消除其机器学习模型的预期使用和部署中的任何不公平性。）
​		Fairlearn 开源包支持下列类型的奇偶校验约束：

| 奇偶校验约束     | 目的                   | 机器学习任务   |
| :--------------- | :--------------------- | :------------- |
| 人口统计奇偶校验 | 缓解分配损害           | 二元分类、回归 |
| 均等几率         | 诊断分配和服务质量损害 | 二元分类       |
| 均等机会         | 诊断分配和服务质量损害 | 二元分类       |
| 有界群体损失     | 缓解服务质量损害       | 回归           |



##### 5.缓解算法

（其中本次实验用到了下面的 ‘GridSearch’ 与 ‘ThresholdOptimizer’ 方法）
Fairlearn 开源包提供了后期处理和降低不公平性的缓解算法：

| 算法                  | 说明                                                         | 机器学习任务 | 敏感特征 | 支持的奇偶校验约束                                           | 算法类型 |
| --------------------- | ------------------------------------------------------------ | ------------ | -------- | ------------------------------------------------------------ | -------- |
| ExponentiatedGradient | 公平分类的约简方法中描述的公平分类的黑盒方法                 | 二分类       | 分类     | [ 人口统计奇偶校验](https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-fairness-ml#parity-constraints)、[均等几率](https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-fairness-ml#parity-constraints) | 约简     |
| GridSearch            | 公平分类的约简方法）中描述的黑盒方法                         | 二分类       | 二进制   | [ 人口统计奇偶校验](https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-fairness-ml#parity-constraints)、[均等几率](https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-fairness-ml#parity-constraints) | 约简     |
| GridSearch            | 一种黑盒方法，它通过[公平回归：量化的定义和基于约简的算法](https://arxiv.org/abs/1905.12843)中描述的用于有界群体损失的算法实现公平回归的网格搜索变体。 | 回归         | 二进制   | [有界群体损失](https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-fairness-ml#parity-constraints) | 约简     |
| ThresholdOptimizer    | 监督式学习中的机会均等性，一文的后期处理算法。 此方法采用现有分类器和敏感特征作为输入，并派生分类器预测的单一转换，以强制实施指定的奇偶校验约束。 | 二分类       | 分类     | [人口统计奇偶校验](https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-fairness-ml#parity-constraints)、[均等几率](https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-fairness-ml#parity-constraints) | 后处理   |



##### 6.ROC&AUC

		因为本次项目是一个二分类问题所以涉及到二分类模型的测评指标，下面介绍ROC与AUC指标。ROC（Receiver Operating Characteristic，接受者工作特征曲线）曲线和AUC常被用来评价一个二值分类器（binary classifier）的优劣。
（1）ROC曲线
		通过调整模型预测的阈值可以得到不同的点，将这些点可以连成一条曲线，这条曲线叫做接受者工作特征曲线(Receiver Operating Characteristic Curve，简称ROC曲线）。
		ROC曲线有一个很好的特征：在实际的数据集中经常会出现类别不平衡现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间而变化。而在这种情况下，ROC曲线能够保持不变。曲线中其中横坐标为FPR（False positive rate 假阳率），纵坐标为真阳率TPR（True postive rate）。**ROC曲线越接近左上角，该分类器的性能越好，**意味着分类器在假阳率很低的同时获得了很高的真阳率。
（2）AUC		
		AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。
		AUC是一个概率值，当随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的分数将这个正样本排在负样本前面的概率就是AUC值。所以，AUC的值越大，当前的分类算法越有可能将正样本排在负样本值前面，既能够更好的分类。

https://blog.csdn.net/pipisorry/article/details/51788927

https://blog.csdn.net/yinyu19950811/article/details/81288287



##### 4.LightGBM







## 三、实验内容概述

- **涉及领域：**
  - 金融贷款方面的决策分析。我们分析的数据是原始数据经过人工简单处理过的，是为了展现准确性方面的悬殊差异。
- **机器学习任务：**
  - 二元性分类
- **机器学习公平任务：**
  - 使用Fairlearn metrics和Fairlearn dashboard来评估模型的公平。
  - 使用Fairlearn中的改进算法来改进模型的公平水平。
- **性能指标：**
  - ROC曲线下的面积。
  - 平衡过后的准确率。
- **公平指标：**
  - Equalized-odds difference.
- **改进的算法:**
  - `fairlearn.reductions.GridSearch`
  - `fairlearn.postprocessing.ThresholdOptimizer`



## 四、数据分析

​		UCI数据集包含30,000名客户及其在台湾一家银行的信用卡交易数据。除了客户的静态特征外，该数据集还包含2005年4月至9月的信用卡账单支付历史，以及客户信用卡的余额限制。目标是客户是否会在接下来的一个月，即2005年10月拖欠信用卡付款。可以想象，在这个数据上训练出来的模型在实践中可以用来确定客户是否有资格获得其他产品，如汽车贷款等。		
​		该数据集包含23个输入变量(input variable)和一个响应变量（response variable）。该数据集来源于UCI machine learning repository,为某银行的信用卡客户信息数据，共有30000个样本，包括过去六个月的账单还款情况。
ID：信用卡客户ID号
LIMIT_BAL：以新台币计算的信贷金额（包括个人和家庭/补充信贷）/ 信用卡限额，会被替换成一个合成的更具典型性的特征。
SEX： 性别 (1代表男性，2代表女性)
EDUCATION：受 教育程度(1=研究生, 2=大学, 3=高中, 4=其他 5=未知, 6=未知)
MARRIAGE：婚姻状况（1=已婚，2=单身，3=其他）
AGE：年龄
X1：信用额度，包括其个人和家庭补充信用
X2：性别（1=male;2=female）
X3：教育（1=研究生，2=大学，3=高中，4=其他）
X4：婚姻状况（1=已婚，2=单身，3=其他）
X5：年龄，age
X6-X11：过去六个月的还款情况。X6-X11为9-4月的还款情况。其中，-1,代表按时还款；1,代表延时一个月还款；2,代表延时两个月还款.......依次类推，XN=n,代表延时n个月还款，
X12-X17：过去六个月的账单数额情况。X12-X17为9-4月账单数额情况
X18-X23：过去六个月的还款数额情况。X18-X23为9-4月还款数额情况
Y：目标属性，客户下个月还款违约情况（1=逾期，0=未逾期）

实践部分：
read_csv读入数据集后
（1）使用 ‘data.info()’ 查看数据整体信息

<img src="实验报告.assets/截屏2021-06-02 下午5.50.21.png" alt="截屏2021-06-02 下午5.50.21" style="zoom:33%;" />

（2）使用 ‘data.describe()’ 查看数据基本情况(包括中位数、最大最小值等)，在此只列出头尾部分

<img src="实验报告.assets/截屏2021-06-02 下午5.51.44.png" alt="截屏2021-06-02 下午5.51.44" style="zoom: 33%;" />

<img src="实验报告.assets/截屏2021-06-02 下午5.52.24.png" alt="截屏2021-06-02 下午5.52.24" style="zoom:33%;" />

（3）使用 ‘data.isnull().sum()' 查看每个属性是否存在缺失值

<img src="实验报告.assets/截屏2021-06-02 下午5.53.10.png" alt="截屏2021-06-02 下午5.53.10" style="zoom:33%;" />

（4）为了使属性对于预测结果的影响更加直观，下面进行数据可视化，对属性进行分析

总体查看下个月违约情况

<img src="实验报告.assets/截屏2021-06-03 下午2.33.42.png" alt="截屏2021-06-03 下午2.33.42" style="zoom:33%;" />

受教育水平分布与违约情况

<img src="实验报告.assets/截屏2021-06-03 下午2.31.48.png" alt="截屏2021-06-03 下午2.31.48" style="zoom: 33%;" />

性别分布与违约情况

<img src="实验报告.assets/截屏2021-06-03 下午2.35.05.png" alt="截屏2021-06-03 下午2.35.05" style="zoom:33%;" />

年龄分布与与违约情况

<img src="实验报告.assets/截屏2021-06-03 下午2.35.48.png" alt="截屏2021-06-03 下午2.35.48" style="zoom:33%;" />

信用额度分配

<img src="实验报告.assets/截屏2021-06-03 下午2.36.10.png" alt="截屏2021-06-03 下午2.36.10" style="zoom:33%;" />







## 五、实验过程

##### 1.魔改特征

​		我们魔改一下信用卡限额信息 `LIMIT_BAL` 使得该特征对于预测女人来说非常“强”而对男人来说相对“弱”。例如，我们可以想象，较高的信用额度表明女性客户违约的可能性较小，但对男性客户的违约概率没有提供任何信息。

```python
dist_scale = 0.5
np.random.seed(12345)
# Make 'LIMIT_BAL' informative of the target
dataset['LIMIT_BAL'] = Y + np.random.normal(scale=dist_scale, size=dataset.shape[0])
# But then make it uninformative for the male clients
dataset.loc[A==1, 'LIMIT_BAL'] = np.random.normal(scale=dist_scale, size=dataset[A==1].shape[0])

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)
# Plot distribution of LIMIT_BAL for men
dataset['LIMIT_BAL'][(A==1) & (Y==0)].plot(kind='kde', label="Payment on Time", ax=ax1, 
                                           title="LIMIT_BAL distribution for men")
dataset['LIMIT_BAL'][(A==1) & (Y==1)].plot(kind='kde', label="Payment Default", ax=ax1)
# Plot distribution of LIMIT_BAL for women
dataset['LIMIT_BAL'][(A==2) & (Y==0)].plot(kind='kde', label="Payment on Time", ax=ax2, 
                                           legend=True, title="LIMIT_BAL distribution for women")
dataset['LIMIT_BAL'][(A==2) & (Y==1)].plot(kind='kde', label="Payment Default", ax=ax2, 
                                           legend=True).legend(bbox_to_anchor=(1.6, 1))
plt.show()
```

<img src="实验报告.assets/截屏2021-06-03 下午2.04.29.png" alt="截屏2021-06-03 下午2.04.29" style="zoom:33%;" />

我们从上面的数字和图像中注意到，新的 ‘LIMIT_BAL’ 功能对女性确实有很高的预测性，但对男性则没有。

##### 2.使用无公平意识的模型

​		我们在修改后的数据上训练一个开箱即用的 ‘lightgbm’ 模型，并评估几个差异指标。

计算AUC值

```python
roc_auc_score(Y_train, model.predict_proba(df_train)[:, 1])
```

<img src="实验报告.assets/截屏2021-06-04 上午10.44.51.png" alt="截屏2021-06-04 上午10.44.51" style="zoom:33%;" />

​		可视化模型中各特征值对重要程度，可以看到合成特征 ‘LIMIT_BAL‘ 在这个模型中作为最重要的特征出现，尽管它对数据中的整个人口段没有预测能力。



<img src="实验报告.assets/截屏2021-06-03 下午2.18.27.png" alt="截屏2021-06-03 下午2.18.27" style="zoom:33%;" />

​		我们注意到人工修改过的特征 `LIMIT_BAL` 作为这个模型中最重要的特征出现，尽管它对数据中的整个人口部分没有预测能力。
计算几个性能和差异指标：

<img src="实验报告.assets/截屏2021-06-03 下午2.21.36.png" alt="截屏2021-06-03 下午2.21.36" style="zoom:33%;" />

​		从上到下依次为：总体选择率、人口统计学上的均等差异、人口统计学上的奇偶比、总体平衡错误率、平衡误差率差、均衡赔率差、总体AUC、AUC差异
​		作为总体性能指标，我们使用ROC曲线下的面积（AUC），它适合于正负例子之间有很大不平衡的分类问题。对于二元分类器来说，这与平衡精度是一样的。
​		作为公平性的衡量标准，我们使用*equalized odds difference*，它量化了不同人口统计学中所经历的准确性的差异。我们的目标是确保两组中的任何一组（男性对女性）的假阳性率或假阴性率都不比另一组大。*equalized odds difference*等于以下两个数字中较大的一个。1）两组的假阳性率之差；（2）两组的假阴性率之差。
​		上表显示总体AUC为0.85（基于连续预测），总体平衡误差率(overall balanced error)为0.22（基于0/1预测）。在我们的应用环境中，这两点都是令人满意的。然而，准确率有很大的差距,如平衡误差率差异(balanced error rate difference)，当我们考虑到均衡赔率差异(equalized-odds difference)时，差距甚至更大。作为理智的检查，我们还显示了人口学的平准率，其水平（略高于0.8）在这种情况下被认为是令人满意的。

##### 3.通过后处理减小Equalized-Odds Difference

​		我们尝试缓解 `lightgbm` 预测中的基于敏感特征的结果悬殊，方法是使用Fairlearn postprocessing算法 `ThresholdOptimizer`，该算法通过在equalized-odds difference（在训练数据上）为零的约束条件下优化准确率，为 `lightgbm` 模型产生的分数（类别概率）找到一个合适的阈值。由于我们的目标是优化均衡准确率，我们对训练数据重新取样，使其具有相同数量的正面和负面例子。这意味着 `ThresholdOptimizer` 对于优化在原数据上取得的平衡准确率是非常有效的。

```python
```

​		`ThresholdOptimizer`算法大大减少了原有的不均衡性。然而，性能指标（平衡错误率以及AUC）变得更糟。在实践中部署这样一个模型之前，重要的是要更详细地研究为什么我们观察到这样的情况。在我们的案例中，这是因为可用的特征对其中一个人口群体的信息量比对另一个人口群体的信息量小得多。
​		跟不管公平的算法相比`ThresholdOptimizer`产生0/1预测, 所以它的平衡误差率之差等于AUC之差，而它的总体平衡误差率等于1-总体AUC。
​		接下来，我们在dashboard中比较`lightgbm`模型和这个模型。作为性能指标，我们可以选择均衡准确率。现在的仪表板并不直接显示均衡的赔率差异，但在准确率差异视图中显示了类似的信息，我们可以检查两组的高预测率和低预测率之间的差异。		

##### 4.用改进 GridSearch 算法改进 Equalized-Odds Difference

​		我们现在尝试使用 `GridSearch` 算法来缓解差异。与 `ThresholdOptimizer` 不同， `GridSearch` 产生的预测器在测试时不访问敏感特征。另外，我们不是训练单一的模型，而是训练与性能指标（平衡精度）和公平指标（均衡赔率差异）之间的不同权衡点相对应的多个模型。

```python
```





## 六、总结





## 七、附录

【1】Azure 机器学习文档：https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-fairness-ml



